# Fine-Tuning a T5-Small Model for Text Summarization

This repository contains code and instructions for fine-tuning a **T5-Small** model to summarize scientific papers using the **scientific_papers** dataset from Hugging Face. The process includes data preprocessing, tokenization, training, and evaluation using **Hugging Face's Transformers** and **Weights & Biases** for tracking.

## Key Highlights:
- Introduction to **T5 (Text-To-Text Transfer Transformer)** and its sequence-to-sequence architecture.
- Step-by-step guide to fine-tuning **T5-Small** for text summarization.
- Dataset preparation using the **scientific_papers** dataset.
- Model training with **Hugging Face Transformers** and evaluation using **ROUGE scores**.
- Deployment of the trained model for inference.


## Full Article
For the full tutorial, check out the original Medium article:
ðŸ‘‰ [Read the full article on Medium](https://medium.com/@abdullahk.sulaiman/can-i-creat-my-own-text-summarization-654252f0b138)

## Online Training
For online training, check out the Kaggle notebook: ðŸ‘‰ [Kaggle notebook here](https://www.kaggle.com/code/abdullahksulaiman/fine-tuning-t5-small)

